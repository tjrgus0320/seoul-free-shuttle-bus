"""
ì—ì´ì „íŠ¸ 1: ìì¹˜êµ¬ ê³µì§€ URL í¬ë¡¤ëŸ¬
- ì„œìš¸ì‹œ ê³µì‹ í˜ì´ì§€ì—ì„œ ìì¹˜êµ¬ë³„ ì…”í‹€ë²„ìŠ¤ ê³µì§€ ìˆ˜ì§‘
- PDF/ì²¨ë¶€íŒŒì¼ URL ìë™ ì¶”ì¶œ
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import json
import logging
from typing import List, Dict, Optional
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent.parent))
from config import CRAWL_CONFIG, OFFICIAL_SOURCES, SEOUL_DISTRICTS, RAW_DIR

logger = logging.getLogger(__name__)


class DistrictCrawler:
    """ìì¹˜êµ¬ ì…”í‹€ë²„ìŠ¤ ê³µì§€ í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": CRAWL_CONFIG["user_agent"],
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8"
        })
        self.collected_data = []
        self.keywords = ["ë¬´ë£Œ", "ì…”í‹€", "íŒŒì—…", "ë¹„ìƒ", "ìˆ˜ì†¡", "ë…¸ì„ ", "ìš´í–‰"]

    def fetch_page(self, url: str) -> Optional[str]:
        """ì›¹ í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(CRAWL_CONFIG["retry_count"]):
            try:
                response = self.session.get(
                    url,
                    timeout=CRAWL_CONFIG["timeout"]
                )
                response.encoding = "utf-8"

                if response.status_code == 200:
                    return response.text

                logger.warning(f"HTTP {response.status_code}: {url}")

            except requests.RequestException as e:
                logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                time.sleep(CRAWL_CONFIG["retry_delay"])

        return None

    def extract_district_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """ìì¹˜êµ¬ ê´€ë ¨ ë§í¬ ì¶”ì¶œ"""
        links = []

        for a_tag in soup.find_all("a", href=True):
            href = a_tag.get("href", "")
            text = a_tag.get_text(strip=True)

            # ìì¹˜êµ¬ ì´ë¦„ í™•ì¸
            for district in SEOUL_DISTRICTS.keys():
                if district in text:
                    full_url = urljoin(base_url, href)
                    links.append({
                        "district": district,
                        "text": text,
                        "url": full_url,
                        "type": "district_link"
                    })
                    break

        return links

    def extract_attachments(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """ì²¨ë¶€íŒŒì¼ (PDF, HWP ë“±) URL ì¶”ì¶œ"""
        attachments = []
        file_extensions = [".pdf", ".hwp", ".hwpx", ".docx", ".xlsx"]

        for a_tag in soup.find_all("a", href=True):
            href = a_tag.get("href", "").lower()

            for ext in file_extensions:
                if ext in href:
                    full_url = urljoin(base_url, a_tag.get("href"))
                    filename = a_tag.get_text(strip=True) or urlparse(href).path.split("/")[-1]

                    attachments.append({
                        "filename": filename,
                        "url": full_url,
                        "type": ext.replace(".", ""),
                        "source": base_url
                    })
                    break

        return attachments

    def extract_route_info(self, soup: BeautifulSoup, source_url: str) -> List[Dict]:
        """í˜ì´ì§€ ë³¸ë¬¸ì—ì„œ ë…¸ì„  ì •ë³´ ì¶”ì¶œ"""
        routes = []

        # ë³¸ë¬¸ ì˜ì—­ ì„ íƒ
        content_areas = soup.select(
            "div.view-con, div.content, article, .post-content, "
            ".board-view, .bbs-view, main"
        )

        if not content_areas:
            content_areas = [soup.body] if soup.body else []

        for section in content_areas:
            text = section.get_text("\n", strip=True)

            # ìì¹˜êµ¬ë³„ ì •ë³´ ì¶”ì¶œ
            for district, info in SEOUL_DISTRICTS.items():
                if district in text:
                    # í•´ë‹¹ êµ¬ ê´€ë ¨ í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ì¶œ
                    lines = text.split("\n")
                    district_lines = []
                    in_district_section = False

                    for line in lines:
                        if district in line:
                            in_district_section = True

                        if in_district_section:
                            district_lines.append(line)

                            # ë‹¤ë¥¸ êµ¬ ì´ë¦„ì´ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
                            other_districts = [d for d in SEOUL_DISTRICTS.keys() if d != district]
                            if any(d in line for d in other_districts) and district not in line:
                                break

                    if district_lines:
                        routes.append({
                            "district": district,
                            "raw_text": "\n".join(district_lines[:50]),  # ìµœëŒ€ 50ì¤„
                            "source": source_url
                        })

        return routes

    def crawl_main_sources(self) -> List[Dict]:
        """ê³µì‹ ì†ŒìŠ¤ í¬ë¡¤ë§"""
        results = []

        for source in OFFICIAL_SOURCES:
            logger.info(f"í¬ë¡¤ë§: {source['name']} ({source['url']})")

            html = self.fetch_page(source["url"])
            if not html:
                continue

            soup = BeautifulSoup(html, "lxml")

            # ìì¹˜êµ¬ ë§í¬ ì¶”ì¶œ
            district_links = self.extract_district_links(soup, source["url"])
            logger.info(f"  ë°œê²¬ëœ ìì¹˜êµ¬ ë§í¬: {len(district_links)}ê°œ")

            # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
            attachments = self.extract_attachments(soup, source["url"])
            logger.info(f"  ë°œê²¬ëœ ì²¨ë¶€íŒŒì¼: {len(attachments)}ê°œ")

            # ë³¸ë¬¸ ë…¸ì„  ì •ë³´ ì¶”ì¶œ
            routes = self.extract_route_info(soup, source["url"])
            logger.info(f"  ì¶”ì¶œëœ ë…¸ì„  ì •ë³´: {len(routes)}ê°œ")

            results.append({
                "source": source,
                "district_links": district_links,
                "attachments": attachments,
                "routes": routes,
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")
            })

            time.sleep(1)  # ì˜ˆì˜ ë°”ë¥¸ í¬ë¡¤ë§

        return results

    def crawl_district_pages(self, district_links: List[Dict]) -> List[Dict]:
        """ìì¹˜êµ¬ ê°œë³„ í˜ì´ì§€ í¬ë¡¤ë§"""
        results = []

        for link in district_links:
            logger.info(f"ìì¹˜êµ¬ í˜ì´ì§€ í¬ë¡¤ë§: {link['district']} ({link['url']})")

            html = self.fetch_page(link["url"])
            if not html:
                continue

            soup = BeautifulSoup(html, "lxml")

            # ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
            attachments = self.extract_attachments(soup, link["url"])

            # ë…¸ì„  ì •ë³´ ì¶”ì¶œ
            routes = self.extract_route_info(soup, link["url"])

            results.append({
                "district": link["district"],
                "url": link["url"],
                "attachments": attachments,
                "routes": routes,
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")
            })

            time.sleep(1)

        return results

    def download_attachment(self, url: str, save_path: Path) -> bool:
        """ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = self.session.get(url, timeout=30, stream=True)

            if response.status_code == 200:
                with open(save_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                logger.info(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {save_path.name}")
                return True

        except Exception as e:
            logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

        return False

    def run(self, download_files: bool = True) -> Dict:
        """ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("=" * 50)
        logger.info("ì—ì´ì „íŠ¸ 1: ìì¹˜êµ¬ ê³µì§€ URL í¬ë¡¤ëŸ¬ ì‹œì‘")
        logger.info("=" * 50)

        # ë©”ì¸ ì†ŒìŠ¤ í¬ë¡¤ë§
        main_results = self.crawl_main_sources()

        # ìì¹˜êµ¬ ë§í¬ ìˆ˜ì§‘
        all_district_links = []
        for result in main_results:
            all_district_links.extend(result.get("district_links", []))

        # ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_links = []
        for link in all_district_links:
            if link["url"] not in seen_urls:
                seen_urls.add(link["url"])
                unique_links.append(link)

        # ìì¹˜êµ¬ í˜ì´ì§€ í¬ë¡¤ë§
        district_results = self.crawl_district_pages(unique_links)

        # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        downloaded_files = []
        if download_files:
            all_attachments = []
            for result in main_results:
                all_attachments.extend(result.get("attachments", []))
            for result in district_results:
                all_attachments.extend(result.get("attachments", []))

            for attachment in all_attachments:
                if attachment["type"] == "pdf":
                    filename = f"{attachment['filename']}"
                    if not filename.endswith(".pdf"):
                        filename += ".pdf"
                    save_path = RAW_DIR / filename

                    if self.download_attachment(attachment["url"], save_path):
                        downloaded_files.append(str(save_path))

        # ê²°ê³¼ ì €ì¥
        output = {
            "main_results": main_results,
            "district_results": district_results,
            "downloaded_files": downloaded_files,
            "summary": {
                "total_sources": len(main_results),
                "total_district_pages": len(district_results),
                "total_attachments": len(downloaded_files),
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")
            }
        }

        output_path = RAW_DIR / "crawl_results.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        logger.info(f"\ní¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼: {output_path}")

        return output


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s"
    )

    crawler = DistrictCrawler()
    results = crawler.run(download_files=True)

    print(f"\nğŸ“Š í¬ë¡¤ë§ ìš”ì•½:")
    print(f"  - ë©”ì¸ ì†ŒìŠ¤: {results['summary']['total_sources']}ê°œ")
    print(f"  - ìì¹˜êµ¬ í˜ì´ì§€: {results['summary']['total_district_pages']}ê°œ")
    print(f"  - ë‹¤ìš´ë¡œë“œ íŒŒì¼: {results['summary']['total_attachments']}ê°œ")
