"""
ì„œìš¸ ë¬´ë£Œ ì…”í‹€ë²„ìŠ¤ ë…¸ì„  ì •ë³´ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
- ì„œìš¸ì‹œ ê³µì‹ í˜ì´ì§€ ë° ìì¹˜êµ¬ ê³µì§€ì—ì„œ ì…”í‹€ë²„ìŠ¤ ì •ë³´ ìˆ˜ì§‘
- HTML ê¸°ë°˜ ê³µì§€ ìë™ ìˆ˜ì§‘ â†’ JSON ë³€í™˜
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime
from typing import List, Dict, Any

# ìˆ˜ì§‘ ëŒ€ìƒ URL ëª©ë¡
SOURCES = [
    {
        "name": "ì„œìš¸ì‹œ ë¹„ìƒìˆ˜ì†¡ëŒ€ì±…",
        "url": "https://news.seoul.go.kr/traffic/archives/514068",
        "type": "official"
    },
    # ì¶”ê°€ ì†ŒìŠ¤ëŠ” ì—¬ê¸°ì— ë“±ë¡
]

def fetch_page(url: str) -> str:
    """ì›¹ í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        res = requests.get(url, headers=headers, timeout=10)
        res.encoding = "utf-8"
        return res.text
    except Exception as e:
        print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {url} - {e}")
        return ""

def extract_district_info(soup: BeautifulSoup, source_url: str) -> List[Dict]:
    """HTMLì—ì„œ ìì¹˜êµ¬ë³„ ì…”í‹€ë²„ìŠ¤ ì •ë³´ ì¶”ì¶œ"""
    districts = []

    # ë³¸ë¬¸ ì˜ì—­ ì„ íƒ (ì‚¬ì´íŠ¸ë§ˆë‹¤ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    content_areas = soup.select("div.view-con, div.content, article, .post-content")

    if not content_areas:
        content_areas = [soup.body] if soup.body else []

    for section in content_areas:
        text = section.get_text("\n", strip=True)

        # ìì¹˜êµ¬ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: ì˜ë“±í¬êµ¬, ê´€ì•…êµ¬ ë“±)
        district_matches = re.findall(r"([ê°€-í£]{1,3}êµ¬)", text)

        # ì„œìš¸ì‹œ ìì¹˜êµ¬ ëª©ë¡ìœ¼ë¡œ í•„í„°ë§
        seoul_districts = [
            "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ìš©ì‚°êµ¬", "ì„±ë™êµ¬", "ê´‘ì§„êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ì¤‘ë‘êµ¬",
            "ì„±ë¶êµ¬", "ê°•ë¶êµ¬", "ë„ë´‰êµ¬", "ë…¸ì›êµ¬", "ì€í‰êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ë§ˆí¬êµ¬",
            "ì–‘ì²œêµ¬", "ê°•ì„œêµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ì˜ë“±í¬êµ¬", "ë™ì‘êµ¬", "ê´€ì•…êµ¬",
            "ì„œì´ˆêµ¬", "ê°•ë‚¨êµ¬", "ì†¡íŒŒêµ¬", "ê°•ë™êµ¬"
        ]

        found_districts = [d for d in district_matches if d in seoul_districts]
        found_districts = list(dict.fromkeys(found_districts))  # ì¤‘ë³µ ì œê±°

        for district_name in found_districts:
            routes = []

            # í•´ë‹¹ êµ¬ ê´€ë ¨ ë…¸ì„  ì •ë³´ ì¶”ì¶œ
            for line in text.split("\n"):
                if district_name in line or "ìš´í–‰" in line or "ë…¸ì„ " in line or "ì…”í‹€" in line:
                    # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
                    time_match = re.search(r"(\d{1,2}:\d{2})\s*[~-]\s*(\d{1,2}:\d{2})", line)
                    # ë°°ì°¨ê°„ê²© ì¶”ì¶œ
                    interval_match = re.search(r"(\d+)\s*[~-]?\s*(\d+)?\s*ë¶„", line)

                    route_info = {
                        "raw_text": line.strip(),
                        "hours": f"{time_match.group(1)}~{time_match.group(2)}" if time_match else None,
                        "interval": f"{interval_match.group(1)}~{interval_match.group(2)}ë¶„" if interval_match and interval_match.group(2) else f"{interval_match.group(1)}ë¶„" if interval_match else None
                    }

                    if route_info["raw_text"]:
                        routes.append(route_info)

            if routes:
                districts.append({
                    "district": district_name,
                    "routes": routes,
                    "source": source_url
                })

    return districts

def collect_all_sources() -> Dict[str, Any]:
    """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
    all_data = {
        "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "sources": [],
        "districts": []
    }

    seen_districts = set()

    for source in SOURCES:
        print(f"ğŸ“¡ ìˆ˜ì§‘ ì¤‘: {source['name']} ({source['url']})")

        html = fetch_page(source["url"])
        if not html:
            continue

        soup = BeautifulSoup(html, "lxml")
        districts = extract_district_info(soup, source["url"])

        all_data["sources"].append({
            "name": source["name"],
            "url": source["url"],
            "collected": True
        })

        for district in districts:
            if district["district"] not in seen_districts:
                all_data["districts"].append(district)
                seen_districts.add(district["district"])
                print(f"  âœ… {district['district']}: {len(district['routes'])}ê°œ ë…¸ì„  ì •ë³´")

    return all_data

def save_raw_data(data: Dict, filename: str = "shuttle_routes_raw.json"):
    """ìˆ˜ì§‘ëœ ì›ë³¸ ë°ì´í„° ì €ì¥"""
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")

def main():
    print("=" * 50)
    print("ğŸšŒ ì„œìš¸ ë¬´ë£Œ ì…”í‹€ë²„ìŠ¤ ë…¸ì„  ì •ë³´ ìˆ˜ì§‘ê¸°")
    print("=" * 50)
    print()

    # ë°ì´í„° ìˆ˜ì§‘
    data = collect_all_sources()

    # ì›ë³¸ ë°ì´í„° ì €ì¥
    save_raw_data(data)

    print()
    print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(data['districts'])}ê°œ ìì¹˜êµ¬ ì •ë³´")
    print("=" * 50)
    print()
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. shuttle_routes_raw.json ê²€í† ")
    print("   2. í•„ìš”ì‹œ ìˆ˜ë™ ë³´ì •")
    print("   3. shuttle_routes.jsonìœ¼ë¡œ ì •ê·œí™”")
    print("   4. index.htmlì—ì„œ í™•ì¸")

if __name__ == "__main__":
    main()
